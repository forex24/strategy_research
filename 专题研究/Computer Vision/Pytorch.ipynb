{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e3d98f6",
   "metadata": {},
   "source": [
    "- Reference: https://www.bilibili.com/video/BV1hE411t7RN?p=14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a617ebe9",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "646e1fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc23706",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset??\n",
    "# __len__(self)\n",
    "# __getitem__(self, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df1207af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "726c9362",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyData(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, label_dir):\n",
    "        self.root_dir = root_dir \n",
    "        self.label_dir = label_dir\n",
    "        self.path = os.path.join(self.root_dir, self.label_dir)\n",
    "        self.img_path = os.listdir(self.path)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_path[idx]\n",
    "        img_item_path = os.path.join(self.root_dir, self.label_dir, img_name)\n",
    "        img = Image.open(img_item_path)\n",
    "        label = self.label_dir\n",
    "        return img, label \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9582bf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "ants_dataset = MyData()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84bd4e8",
   "metadata": {},
   "source": [
    "# TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7433bfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dfe474",
   "metadata": {},
   "source": [
    "## add_scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05285e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(\"logs\")\n",
    "\n",
    "#writer.add_image()\n",
    "for i in range(100):\n",
    "    writer.add_scalar(tag='y=x', scalar_value=i, global_step=i)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "970212f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=logs --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cc09ae",
   "metadata": {},
   "source": [
    "## add_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12ea9983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "img_path = './data/train/ants_image/0013035.jpg'\n",
    "img = Image.open(img_path)\n",
    "\n",
    "import numpy as np \n",
    "img_arr = np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e26a42f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_image('test', img_arr, 2, dataformats='HWC')\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13d76b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=logs --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a879f17d",
   "metadata": {},
   "source": [
    "# transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e75cb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "img_path = './data/train/ants_image/0013035.jpg'\n",
    "img = Image.open(img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4e5c14",
   "metadata": {},
   "source": [
    "## ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21b98782",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3137, 0.3137, 0.3137,  ..., 0.3176, 0.3098, 0.2980],\n",
       "         [0.3176, 0.3176, 0.3176,  ..., 0.3176, 0.3098, 0.2980],\n",
       "         [0.3216, 0.3216, 0.3216,  ..., 0.3137, 0.3098, 0.3020],\n",
       "         ...,\n",
       "         [0.3412, 0.3412, 0.3373,  ..., 0.1725, 0.3725, 0.3529],\n",
       "         [0.3412, 0.3412, 0.3373,  ..., 0.3294, 0.3529, 0.3294],\n",
       "         [0.3412, 0.3412, 0.3373,  ..., 0.3098, 0.3059, 0.3294]],\n",
       "\n",
       "        [[0.5922, 0.5922, 0.5922,  ..., 0.5961, 0.5882, 0.5765],\n",
       "         [0.5961, 0.5961, 0.5961,  ..., 0.5961, 0.5882, 0.5765],\n",
       "         [0.6000, 0.6000, 0.6000,  ..., 0.5922, 0.5882, 0.5804],\n",
       "         ...,\n",
       "         [0.6275, 0.6275, 0.6235,  ..., 0.3608, 0.6196, 0.6157],\n",
       "         [0.6275, 0.6275, 0.6235,  ..., 0.5765, 0.6275, 0.5961],\n",
       "         [0.6275, 0.6275, 0.6235,  ..., 0.6275, 0.6235, 0.6314]],\n",
       "\n",
       "        [[0.9137, 0.9137, 0.9137,  ..., 0.9176, 0.9098, 0.8980],\n",
       "         [0.9176, 0.9176, 0.9176,  ..., 0.9176, 0.9098, 0.8980],\n",
       "         [0.9216, 0.9216, 0.9216,  ..., 0.9137, 0.9098, 0.9020],\n",
       "         ...,\n",
       "         [0.9294, 0.9294, 0.9255,  ..., 0.5529, 0.9216, 0.8941],\n",
       "         [0.9294, 0.9294, 0.9255,  ..., 0.8863, 1.0000, 0.9137],\n",
       "         [0.9294, 0.9294, 0.9255,  ..., 0.9490, 0.9804, 0.9137]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_trans = transforms.ToTensor()\n",
    "tensor_img = tensor_trans(img)\n",
    "\n",
    "tensor_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99ee6fb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[233, 151,  80],\n",
       "        [233, 151,  80],\n",
       "        [233, 151,  80],\n",
       "        ...,\n",
       "        [234, 152,  81],\n",
       "        [232, 150,  79],\n",
       "        [229, 147,  76]],\n",
       "\n",
       "       [[234, 152,  81],\n",
       "        [234, 152,  81],\n",
       "        [234, 152,  81],\n",
       "        ...,\n",
       "        [234, 152,  81],\n",
       "        [232, 150,  79],\n",
       "        [229, 147,  76]],\n",
       "\n",
       "       [[235, 153,  82],\n",
       "        [235, 153,  82],\n",
       "        [235, 153,  82],\n",
       "        ...,\n",
       "        [233, 151,  80],\n",
       "        [232, 150,  79],\n",
       "        [230, 148,  77]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[237, 160,  87],\n",
       "        [237, 160,  87],\n",
       "        [236, 159,  86],\n",
       "        ...,\n",
       "        [141,  92,  44],\n",
       "        [235, 158,  95],\n",
       "        [228, 157,  90]],\n",
       "\n",
       "       [[237, 160,  87],\n",
       "        [237, 160,  87],\n",
       "        [236, 159,  86],\n",
       "        ...,\n",
       "        [226, 147,  84],\n",
       "        [255, 160,  90],\n",
       "        [233, 152,  84]],\n",
       "\n",
       "       [[237, 160,  87],\n",
       "        [237, 160,  87],\n",
       "        [236, 159,  86],\n",
       "        ...,\n",
       "        [242, 160,  79],\n",
       "        [250, 159,  78],\n",
       "        [233, 161,  84]]], dtype=uint8)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "cv_img = cv2.imread(img_path)\n",
    "cv_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "012ec729",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ad1eb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('logs')\n",
    "writer.add_image('tensor_img', tensor_img)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "891f773e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=logs --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979111c1",
   "metadata": {},
   "source": [
    "## Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be2c58df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3725, -0.3725, -0.3725,  ..., -0.3647, -0.3804, -0.4039],\n",
       "         [-0.3647, -0.3647, -0.3647,  ..., -0.3647, -0.3804, -0.4039],\n",
       "         [-0.3569, -0.3569, -0.3569,  ..., -0.3725, -0.3804, -0.3961],\n",
       "         ...,\n",
       "         [-0.3176, -0.3176, -0.3255,  ..., -0.6549, -0.2549, -0.2941],\n",
       "         [-0.3176, -0.3176, -0.3255,  ..., -0.3412, -0.2941, -0.3412],\n",
       "         [-0.3176, -0.3176, -0.3255,  ..., -0.3804, -0.3882, -0.3412]],\n",
       "\n",
       "        [[ 0.1843,  0.1843,  0.1843,  ...,  0.1922,  0.1765,  0.1529],\n",
       "         [ 0.1922,  0.1922,  0.1922,  ...,  0.1922,  0.1765,  0.1529],\n",
       "         [ 0.2000,  0.2000,  0.2000,  ...,  0.1843,  0.1765,  0.1608],\n",
       "         ...,\n",
       "         [ 0.2549,  0.2549,  0.2471,  ..., -0.2784,  0.2392,  0.2314],\n",
       "         [ 0.2549,  0.2549,  0.2471,  ...,  0.1529,  0.2549,  0.1922],\n",
       "         [ 0.2549,  0.2549,  0.2471,  ...,  0.2549,  0.2471,  0.2627]],\n",
       "\n",
       "        [[ 0.8275,  0.8275,  0.8275,  ...,  0.8353,  0.8196,  0.7961],\n",
       "         [ 0.8353,  0.8353,  0.8353,  ...,  0.8353,  0.8196,  0.7961],\n",
       "         [ 0.8431,  0.8431,  0.8431,  ...,  0.8275,  0.8196,  0.8039],\n",
       "         ...,\n",
       "         [ 0.8588,  0.8588,  0.8510,  ...,  0.1059,  0.8431,  0.7882],\n",
       "         [ 0.8588,  0.8588,  0.8510,  ...,  0.7725,  1.0000,  0.8275],\n",
       "         [ 0.8588,  0.8588,  0.8510,  ...,  0.8980,  0.9608,  0.8275]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_norm = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "img_norm = trans_norm(tensor_img)\n",
    "img_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f4d415c",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('logs')\n",
    "writer.add_image('norm_img', img_norm)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b916b756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=logs --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e95730e",
   "metadata": {},
   "source": [
    "## Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "224b6c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method1 （PIL->resize PIL->resize tensor）\n",
    "trans_resize = transforms.Resize((512, 512))\n",
    "img_resize = trans_resize(img)\n",
    "img_resize = tensor_trans(img_resize)\n",
    "\n",
    "# Method1 (高版本可以直接输入tensor了)\n",
    "trans_resize = transforms.Resize((512, 512))\n",
    "img_resize_1 = trans_resize(tensor_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f01d8bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method2\n",
    "trans_resize_2 = transforms.Resize((512, 512))\n",
    "trans_compose = transforms.Compose([trans_resize_2, tensor_trans])\n",
    "img_resize_2 = trans_compose(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955da4c3",
   "metadata": {},
   "source": [
    "## RandomCrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe18dc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_random = transforms.RandomCrop(size=(512, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3373fbbe",
   "metadata": {},
   "source": [
    "# torchvision.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cc947e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.CIFAR10(root='./data/CIFAR10', train=True, download=True)\n",
    "test_set = torchvision.datasets.CIFAR10(root='./data/CIFAR10', train=False, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e904a0",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116d0d4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e229a388",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "167.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
